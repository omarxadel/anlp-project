{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7345136,"sourceType":"datasetVersion","datasetId":4265092}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# FLAN-T5 Prefix-Tuning\nThis notebook is made to prefix-tune FLAN-T5 on Hateeval.","metadata":{}},{"cell_type":"markdown","source":"### 1. Imports and Constants","metadata":{}},{"cell_type":"code","source":"import torch\ntorch.cuda.is_available()","metadata":{"execution":{"iopub.status.busy":"2024-01-09T02:26:03.384542Z","iopub.execute_input":"2024-01-09T02:26:03.384917Z","iopub.status.idle":"2024-01-09T02:26:03.391276Z","shell.execute_reply.started":"2024-01-09T02:26:03.384888Z","shell.execute_reply":"2024-01-09T02:26:03.390440Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"!pip install peft","metadata":{"execution":{"iopub.status.busy":"2024-01-09T02:26:03.393153Z","iopub.execute_input":"2024-01-09T02:26:03.393534Z","iopub.status.idle":"2024-01-09T02:26:16.093434Z","shell.execute_reply.started":"2024-01-09T02:26:03.393500Z","shell.execute_reply":"2024-01-09T02:26:16.092326Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Requirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (0.7.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.1)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.0.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.36.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.1)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.25.0)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.1)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.19.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.12.2)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2023.12.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.0.9)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.8.8)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.15.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2023.11.17)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, default_data_collator, get_linear_schedule_with_warmup\nfrom peft import get_peft_config, get_peft_model, get_peft_model_state_dict, PrefixTuningConfig, TaskType\nimport os\nfrom datasets import Dataset,DatasetDict, load_dataset\nfrom torch.utils.data import DataLoader\nimport torch\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom datasets import Features, ClassLabel\n\nfeatures = ['Non-Hate','Hate']\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n\ndevice = \"cuda\"\nmodel_name_or_path = \"google/flan-t5-xl\"\ntokenizer_name_or_path = \"google/flan-t5-xl\"\n\ntext_column = \"text\"\nlabel_column = \"text_label\"\nmax_length = 128\nlr = 1e-2\nnum_epochs = 5\nbatch_size = 8","metadata":{"execution":{"iopub.status.busy":"2024-01-09T02:26:16.095826Z","iopub.execute_input":"2024-01-09T02:26:16.096146Z","iopub.status.idle":"2024-01-09T02:26:16.104756Z","shell.execute_reply.started":"2024-01-09T02:26:16.096116Z","shell.execute_reply":"2024-01-09T02:26:16.103770Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"### 2. Loading Dataset","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/hateeval/hateeval_preprocessed_data.csv')\ndf = df.dropna()\ntrain, test = train_test_split(df, test_size=0.2)\ntrain = Dataset.from_pandas(train)\ntest = Dataset.from_pandas(test)\nds_dict = {'train' : train,\n           'test' : test}\ndataset = DatasetDict(ds_dict)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T02:26:16.105952Z","iopub.execute_input":"2024-01-09T02:26:16.106212Z","iopub.status.idle":"2024-01-09T02:26:16.165079Z","shell.execute_reply.started":"2024-01-09T02:26:16.106189Z","shell.execute_reply":"2024-01-09T02:26:16.164229Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"### 3. Preprocessing Data\nInitialize a tokenizer, and create a function to pad and truncate the model_inputs and labels:","metadata":{}},{"cell_type":"code","source":"dataset = dataset.map(\n    lambda x: {\"text_label\": [features[label] for label in x[\"HS\"]]},\n    batched=True,\n    num_proc=1,\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T02:26:16.167488Z","iopub.execute_input":"2024-01-09T02:26:16.167802Z","iopub.status.idle":"2024-01-09T02:26:16.274318Z","shell.execute_reply.started":"2024-01-09T02:26:16.167758Z","shell.execute_reply":"2024-01-09T02:26:16.273409Z"},"trusted":true},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/8 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4545c562569e456caa852cdd634505bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94d6fb0a079e48fb8014c7a96d0ca0a2"}},"metadata":{}}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n\n\ndef preprocess_function(examples):\n    inputs = examples[text_column]\n    targets = examples[label_column]\n    model_inputs = tokenizer(inputs, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n    model_inputs['input_ids'] = model_inputs['input_ids'].reshape(max_length)\n    labels = tokenizer(targets, max_length=3, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n    labels = labels[\"input_ids\"]\n    labels[labels == tokenizer.pad_token_id] = -100\n    model_inputs[\"labels\"] = labels.reshape(3)\n    return model_inputs","metadata":{"execution":{"iopub.status.busy":"2024-01-09T02:26:16.275566Z","iopub.execute_input":"2024-01-09T02:26:16.275850Z","iopub.status.idle":"2024-01-09T02:26:16.496331Z","shell.execute_reply.started":"2024-01-09T02:26:16.275825Z","shell.execute_reply":"2024-01-09T02:26:16.495435Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"Use the map function to apply the preprocess_function to the dataset. You can remove the unprocessed columns since the model doesn’t need them anymore:","metadata":{}},{"cell_type":"code","source":"processed_datasets = dataset.map(\n    preprocess_function,\n    num_proc=1,\n    remove_columns=dataset[\"train\"].column_names,\n    load_from_cache_file=False,\n    desc=\"Running tokenizer on dataset\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T02:26:16.498726Z","iopub.execute_input":"2024-01-09T02:26:16.499052Z","iopub.status.idle":"2024-01-09T02:26:24.130219Z","shell.execute_reply.started":"2024-01-09T02:26:16.499024Z","shell.execute_reply":"2024-01-09T02:26:24.129408Z"},"trusted":true},"execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":"Running tokenizer on dataset:   0%|          | 0/7982 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8b8dde11e5c47218bad81203a65b615"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on dataset:   0%|          | 0/1996 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d0e975a4d634c85ba0b27e637160a8e"}},"metadata":{}}]},{"cell_type":"markdown","source":"Create a DataLoader from the train and eval datasets. Set pin_memory=True to speed up the data transfer to the GPU during training if the samples in your dataset are on a CPU.","metadata":{}},{"cell_type":"code","source":"train_dataset = processed_datasets[\"train\"]\neval_dataset = processed_datasets[\"test\"]\ntrain_dataloader = DataLoader(\n    train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True\n)\neval_dataloader = DataLoader(eval_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T02:26:24.131344Z","iopub.execute_input":"2024-01-09T02:26:24.131673Z","iopub.status.idle":"2024-01-09T02:26:24.137409Z","shell.execute_reply.started":"2024-01-09T02:26:24.131648Z","shell.execute_reply":"2024-01-09T02:26:24.136448Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"### 4. Train model\nNow you can setup your model and make sure it is ready for training. Specify the task in PrefixTuningConfig, create the base t5-large model from AutoModelForSeq2SeqLM, and then wrap the model and configuration in a PeftModel. Feel free to print the PeftModel’s parameters and compare it to fully training all the model parameters to see how much more efficient it is!","metadata":{}},{"cell_type":"code","source":"peft_config = PrefixTuningConfig(task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, num_virtual_tokens=20)\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-01-09T02:26:24.138754Z","iopub.execute_input":"2024-01-09T02:26:24.139034Z","iopub.status.idle":"2024-01-09T02:26:31.583561Z","shell.execute_reply.started":"2024-01-09T02:26:24.139010Z","shell.execute_reply":"2024-01-09T02:26:31.582423Z"},"trusted":true},"execution_count":34,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e53e127c206436abbd8802add0f395d"}},"metadata":{}},{"name":"stdout","text":"trainable params: 1,966,080 || all params: 2,851,723,264 || trainable%: 0.06894357614638445\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Setup the optimizer and learning rate scheduler:\r\n","metadata":{}},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\nlr_scheduler = get_linear_schedule_with_warmup(\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=(len(train_dataloader) * num_epochs),\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T02:26:31.584794Z","iopub.execute_input":"2024-01-09T02:26:31.585132Z","iopub.status.idle":"2024-01-09T02:26:31.598363Z","shell.execute_reply.started":"2024-01-09T02:26:31.585101Z","shell.execute_reply":"2024-01-09T02:26:31.597420Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"Move the model to the GPU, and then write a training loop to begin!\r\n","metadata":{}},{"cell_type":"code","source":"model = model.to(device)\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n    for step, batch in enumerate(tqdm(train_dataloader)):\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        total_loss += loss.detach().float()\n        loss.backward()\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n\n    model.eval()\n    eval_loss = 0\n    eval_preds = []\n    for step, batch in enumerate(tqdm(eval_dataloader)):\n        batch = {k: v.to(device) for k, v in batch.items()}\n        with torch.no_grad():\n            outputs = model(**batch)\n        loss = outputs.loss\n        eval_loss += loss.detach().float()\n        eval_preds.extend(\n            tokenizer.batch_decode(torch.argmax(outputs.logits, -1).detach().cpu().numpy(), skip_special_tokens=True)\n        )\n\n    eval_epoch_loss = eval_loss / len(eval_dataloader)\n    eval_ppl = torch.exp(eval_epoch_loss)\n    train_epoch_loss = total_loss / len(train_dataloader)\n    train_ppl = torch.exp(train_epoch_loss)\n    print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}\")","metadata":{"execution":{"iopub.status.busy":"2024-01-09T02:26:31.602124Z","iopub.execute_input":"2024-01-09T02:26:31.603145Z","iopub.status.idle":"2024-01-09T04:11:31.443691Z","shell.execute_reply.started":"2024-01-09T02:26:31.603111Z","shell.execute_reply":"2024-01-09T04:11:31.442727Z"},"trusted":true},"execution_count":36,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/998 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"937d2979427d48faba6ac72ab5c3eb97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0df79bbbac4c44a0b5fd63ce0be803f8"}},"metadata":{}},{"name":"stdout","text":"epoch=0: train_ppl=tensor(1.3115, device='cuda:0') train_epoch_loss=tensor(0.2711, device='cuda:0') eval_ppl=tensor(1.2026, device='cuda:0') eval_epoch_loss=tensor(0.1845, device='cuda:0')\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/998 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88fe9c126c7842a0bc2dfaf379fe1b9f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04ed7a4116104189b3b08efca1931a9a"}},"metadata":{}},{"name":"stdout","text":"epoch=1: train_ppl=tensor(1.2042, device='cuda:0') train_epoch_loss=tensor(0.1858, device='cuda:0') eval_ppl=tensor(1.1601, device='cuda:0') eval_epoch_loss=tensor(0.1485, device='cuda:0')\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/998 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dba22e763e874e3cb8a6a8543063a85d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdf38df46044410fb9b7fe51beed782e"}},"metadata":{}},{"name":"stdout","text":"epoch=2: train_ppl=tensor(1.1818, device='cuda:0') train_epoch_loss=tensor(0.1670, device='cuda:0') eval_ppl=tensor(1.1471, device='cuda:0') eval_epoch_loss=tensor(0.1373, device='cuda:0')\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/998 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a7d1309db5548aabb9552d7eeb5f219"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0e14949686940018b06e17809d096f0"}},"metadata":{}},{"name":"stdout","text":"epoch=3: train_ppl=tensor(1.1660, device='cuda:0') train_epoch_loss=tensor(0.1535, device='cuda:0') eval_ppl=tensor(1.1499, device='cuda:0') eval_epoch_loss=tensor(0.1397, device='cuda:0')\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/998 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e538cb05a8634a0293251c8fa9cccb95"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a52117029aed4d6bad64183e37eeffb4"}},"metadata":{}},{"name":"stdout","text":"epoch=4: train_ppl=tensor(1.1624, device='cuda:0') train_epoch_loss=tensor(0.1504, device='cuda:0') eval_ppl=tensor(1.1418, device='cuda:0') eval_epoch_loss=tensor(0.1326, device='cuda:0')\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let’s see how well the model performs on the validation set:\r\n","metadata":{}},{"cell_type":"code","source":"correct = 0\ntotal = 0\nfor pred, true in zip(eval_preds, dataset[\"test\"][\"text_label\"]):\n    if pred.strip() == true.strip():\n        correct += 1\n    total += 1\naccuracy = correct / total * 100\nprint(f\"{accuracy=} % on the evaluation dataset\")\nprint(f\"{eval_preds[:10]=}\")\nprint(f\"{dataset['test']['text_label'][:10]=}\")","metadata":{"execution":{"iopub.status.busy":"2024-01-09T04:11:31.444940Z","iopub.execute_input":"2024-01-09T04:11:31.445251Z","iopub.status.idle":"2024-01-09T04:11:31.459353Z","shell.execute_reply.started":"2024-01-09T04:11:31.445225Z","shell.execute_reply":"2024-01-09T04:11:31.458441Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"accuracy=35.721442885771545 % on the evaluation dataset\neval_preds[:10]=['Hat-', 'Hate', 'Hat-', 'Non-', 'Non-', 'Non-', 'Non-', 'Non-', 'Hate', 'Non-']\ndataset['test']['text_label'][:10]=['Non-Hate', 'Hate', 'Non-Hate', 'Non-Hate', 'Non-Hate', 'Non-Hate', 'Non-Hate', 'Non-Hate', 'Hate', 'Non-Hate']\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-01-09T04:19:35.366959Z","iopub.execute_input":"2024-01-09T04:19:35.367761Z","iopub.status.idle":"2024-01-09T04:19:35.397734Z","shell.execute_reply.started":"2024-01-09T04:19:35.367730Z","shell.execute_reply":"2024-01-09T04:19:35.396690Z"},"trusted":true},"execution_count":38,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e23a76b7d4c421bb137feec73249f6c"}},"metadata":{}}]},{"cell_type":"code","source":"peft_model_id = \"omarxadel/flan-t5-xl-hs-prefix-tuning\"\nmodel.push_to_hub(peft_model_id, use_auth_token=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T04:20:42.327897Z","iopub.execute_input":"2024-01-09T04:20:42.328267Z","iopub.status.idle":"2024-01-09T04:20:43.710571Z","shell.execute_reply.started":"2024-01-09T04:20:42.328237Z","shell.execute_reply":"2024-01-09T04:20:43.709668Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:821: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/7.86M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a8b1eee0b094effb1ce0a998584cbc5"}},"metadata":{}},{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/omarxadel/flan-t5-xl-hs-prefix-tuning/commit/dd3a60ca5c6552cd33ea48ce1d901b49e768cea3', commit_message='Upload model', commit_description='', oid='dd3a60ca5c6552cd33ea48ce1d901b49e768cea3', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]}]}