{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7345136,"sourceType":"datasetVersion","datasetId":4265092}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# FLAN-T5 Prefix-Tuning\nThis notebook is made to prefix-tune FLAN-T5 on Hateeval.","metadata":{}},{"cell_type":"markdown","source":"### 1. Imports and Constants","metadata":{}},{"cell_type":"code","source":"import torch\ntorch.cuda.is_available()","metadata":{"execution":{"iopub.status.busy":"2024-01-11T08:29:04.282473Z","iopub.execute_input":"2024-01-11T08:29:04.282758Z","iopub.status.idle":"2024-01-11T08:29:07.604097Z","shell.execute_reply.started":"2024-01-11T08:29:04.282734Z","shell.execute_reply":"2024-01-11T08:29:07.602981Z"},"trusted":true},"execution_count":1,"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"!pip install peft","metadata":{"execution":{"iopub.status.busy":"2024-01-11T08:29:07.605946Z","iopub.execute_input":"2024-01-11T08:29:07.606445Z","iopub.status.idle":"2024-01-11T08:29:20.973780Z","shell.execute_reply.started":"2024-01-11T08:29:07.606390Z","shell.execute_reply":"2024-01-11T08:29:20.972732Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting peft\n  Obtaining dependency information for peft from https://files.pythonhosted.org/packages/8b/1b/aee2a330d050c493642d59ba6af51f3910cb138ea48ede228c84c204a5af/peft-0.7.1-py3-none-any.whl.metadata\n  Downloading peft-0.7.1-py3-none-any.whl.metadata (25 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.1)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.0.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.36.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.1)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.25.0)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.1)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.20.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.12.2)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2023.12.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.0.9)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.8.8)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.15.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2023.11.17)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.7.1-py3-none-any.whl (168 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.7.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, default_data_collator, get_linear_schedule_with_warmup\nfrom peft import get_peft_config, get_peft_model, get_peft_model_state_dict, PrefixTuningConfig, TaskType\nimport os\nfrom datasets import Dataset,DatasetDict, load_dataset\nfrom torch.utils.data import DataLoader\nimport torch\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom datasets import Features, ClassLabel\n\nfeatures = ['Non-Hate','Hate Speech']\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n\ndevice = \"cuda\"\nmodel_name_or_path = \"google/flan-t5-xl\"\ntokenizer_name_or_path = \"google/flan-t5-xl\"\n\ntext_column = \"text\"\nlabel_column = \"text_label\"\nmax_length = 128\nlr = 1e-2\nnum_epochs = 5\nbatch_size = 8","metadata":{"execution":{"iopub.status.busy":"2024-01-11T08:29:20.975123Z","iopub.execute_input":"2024-01-11T08:29:20.975417Z","iopub.status.idle":"2024-01-11T08:29:34.532243Z","shell.execute_reply.started":"2024-01-11T08:29:20.975393Z","shell.execute_reply":"2024-01-11T08:29:34.531462Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 2. Loading Dataset","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/hateeval/hateeval_preprocessed_data.csv')\ndf = df.dropna()\ntrain, test = train_test_split(df, test_size=0.2)\ntrain = Dataset.from_pandas(train)\ntest = Dataset.from_pandas(test)\nds_dict = {'train' : train,\n           'test' : test}\ndataset = DatasetDict(ds_dict)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T08:29:34.534414Z","iopub.execute_input":"2024-01-11T08:29:34.535016Z","iopub.status.idle":"2024-01-11T08:29:34.623988Z","shell.execute_reply.started":"2024-01-11T08:29:34.534989Z","shell.execute_reply":"2024-01-11T08:29:34.623036Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### 3. Preprocessing Data\nInitialize a tokenizer, and create a function to pad and truncate the model_inputs and labels:","metadata":{}},{"cell_type":"code","source":"dataset = dataset.map(\n    lambda x: {\"text_label\": [features[label] for label in x[\"HS\"]]},\n    batched=True,\n    num_proc=1,\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T08:29:34.625109Z","iopub.execute_input":"2024-01-11T08:29:34.625385Z","iopub.status.idle":"2024-01-11T08:29:34.752816Z","shell.execute_reply.started":"2024-01-11T08:29:34.625360Z","shell.execute_reply":"2024-01-11T08:29:34.751940Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/8 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfd06dd1b5094eb28fb15cdca3f7e4e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"353233c6179a4dee9415e94d438c3bb5"}},"metadata":{}}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n\n\ndef preprocess_function(examples):\n    inputs = examples[text_column]\n    targets = examples[label_column]\n    model_inputs = tokenizer(inputs, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n    model_inputs['input_ids'] = model_inputs['input_ids'].reshape(max_length)\n    labels = tokenizer(targets, max_length=5, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n    labels = labels[\"input_ids\"]\n    labels[labels == tokenizer.pad_token_id] = -100\n    model_inputs[\"labels\"] = labels.reshape(5)\n    return model_inputs","metadata":{"execution":{"iopub.status.busy":"2024-01-11T08:29:34.753988Z","iopub.execute_input":"2024-01-11T08:29:34.754266Z","iopub.status.idle":"2024-01-11T08:29:36.269427Z","shell.execute_reply.started":"2024-01-11T08:29:34.754241Z","shell.execute_reply":"2024-01-11T08:29:36.268642Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edcff0f0af5b4ef9b2f2c13e1421d541"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73d01a7cb9634ab2aac9140f8a39e4e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"121ebd54db834b71a7a6c2e9013412af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e75d4b20d88541f4b92cb996fb214940"}},"metadata":{}}]},{"cell_type":"markdown","source":"Use the map function to apply the preprocess_function to the dataset. You can remove the unprocessed columns since the model doesn’t need them anymore:","metadata":{}},{"cell_type":"code","source":"processed_datasets = dataset.map(\n    preprocess_function,\n    num_proc=1,\n    remove_columns=dataset[\"train\"].column_names,\n    load_from_cache_file=False,\n    desc=\"Running tokenizer on dataset\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T08:29:36.270554Z","iopub.execute_input":"2024-01-11T08:29:36.270839Z","iopub.status.idle":"2024-01-11T08:29:43.424110Z","shell.execute_reply.started":"2024-01-11T08:29:36.270816Z","shell.execute_reply":"2024-01-11T08:29:43.423338Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Running tokenizer on dataset:   0%|          | 0/7982 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b659f797b09499594e6aee53d0d6abb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on dataset:   0%|          | 0/1996 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48d409386e2b4010b5454a7af076cfa5"}},"metadata":{}}]},{"cell_type":"markdown","source":"Create a DataLoader from the train and eval datasets. Set pin_memory=True to speed up the data transfer to the GPU during training if the samples in your dataset are on a CPU.","metadata":{}},{"cell_type":"code","source":"train_dataset = processed_datasets[\"train\"]\neval_dataset = processed_datasets[\"test\"]\ntrain_dataloader = DataLoader(\n    train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True\n)\neval_dataloader = DataLoader(eval_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T08:29:43.425132Z","iopub.execute_input":"2024-01-11T08:29:43.425383Z","iopub.status.idle":"2024-01-11T08:29:43.432359Z","shell.execute_reply.started":"2024-01-11T08:29:43.425362Z","shell.execute_reply":"2024-01-11T08:29:43.431472Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### 4. Train model\nNow you can setup your model and make sure it is ready for training. Specify the task in PrefixTuningConfig, create the base t5-large model from AutoModelForSeq2SeqLM, and then wrap the model and configuration in a PeftModel. Feel free to print the PeftModel’s parameters and compare it to fully training all the model parameters to see how much more efficient it is!","metadata":{}},{"cell_type":"code","source":"peft_config = PrefixTuningConfig(task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, num_virtual_tokens=20)\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-01-11T08:29:43.433661Z","iopub.execute_input":"2024-01-11T08:29:43.434305Z","iopub.status.idle":"2024-01-11T08:30:50.658135Z","shell.execute_reply.started":"2024-01-11T08:29:43.434271Z","shell.execute_reply":"2024-01-11T08:30:50.656934Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.44k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9456462d6524c1f8834c8093209c4e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/53.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"554498318a9a42808e3b6307ee5f9dd1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8230f0f74e714a0199e0f8f0de876dd8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.45G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a41e4a8e3e1c423981a22fa9cedf77da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f11eab646a14317af9069d740b0c33d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f5bf150965245e3bad62d591b359c12"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e386d4c37a434e61b4935422a56cfe1f"}},"metadata":{}},{"name":"stdout","text":"trainable params: 1,966,080 || all params: 2,851,723,264 || trainable%: 0.06894357614638445\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Setup the optimizer and learning rate scheduler:\r\n","metadata":{}},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\nlr_scheduler = get_linear_schedule_with_warmup(\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=(len(train_dataloader) * num_epochs),\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T08:30:50.661666Z","iopub.execute_input":"2024-01-11T08:30:50.662027Z","iopub.status.idle":"2024-01-11T08:30:50.674933Z","shell.execute_reply.started":"2024-01-11T08:30:50.661994Z","shell.execute_reply":"2024-01-11T08:30:50.673746Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Move the model to the GPU, and then write a training loop to begin!\r\n","metadata":{}},{"cell_type":"code","source":"model = model.to(device)\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n    for step, batch in enumerate(tqdm(train_dataloader)):\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        total_loss += loss.detach().float()\n        loss.backward()\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n\n    model.eval()\n    eval_loss = 0\n    eval_preds = []\n    for step, batch in enumerate(tqdm(eval_dataloader)):\n        batch = {k: v.to(device) for k, v in batch.items()}\n        with torch.no_grad():\n            outputs = model(**batch)\n        loss = outputs.loss\n        eval_loss += loss.detach().float()\n        eval_preds.extend(\n            tokenizer.batch_decode(torch.argmax(outputs.logits, -1).detach().cpu().numpy(), skip_special_tokens=True)\n        )\n\n    eval_epoch_loss = eval_loss / len(eval_dataloader)\n    eval_ppl = torch.exp(eval_epoch_loss)\n    train_epoch_loss = total_loss / len(train_dataloader)\n    train_ppl = torch.exp(train_epoch_loss)\n    print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}\")","metadata":{"execution":{"iopub.status.busy":"2024-01-11T08:30:50.676492Z","iopub.execute_input":"2024-01-11T08:30:50.677183Z","iopub.status.idle":"2024-01-11T09:21:32.694973Z","shell.execute_reply.started":"2024-01-11T08:30:50.677148Z","shell.execute_reply":"2024-01-11T09:21:32.694103Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/998 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8498ff15d17340489ad319a2d256bf8c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9baa618ea29423f8f96a406cdcd3816"}},"metadata":{}},{"name":"stdout","text":"epoch=0: train_ppl=tensor(1.2123, device='cuda:0') train_epoch_loss=tensor(0.1925, device='cuda:0') eval_ppl=tensor(1.1336, device='cuda:0') eval_epoch_loss=tensor(0.1254, device='cuda:0')\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/998 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd66cf55197440dfb82dd379ba532ed8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59a81051c49146998707e9b045186d9e"}},"metadata":{}},{"name":"stdout","text":"epoch=1: train_ppl=tensor(1.1291, device='cuda:0') train_epoch_loss=tensor(0.1214, device='cuda:0') eval_ppl=tensor(1.1114, device='cuda:0') eval_epoch_loss=tensor(0.1056, device='cuda:0')\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/998 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fc5a89eb78d4b1c86bec77b85ad7f35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"094b1ed7f8104ff8a8c462c4377c8a6d"}},"metadata":{}},{"name":"stdout","text":"epoch=2: train_ppl=tensor(1.1153, device='cuda:0') train_epoch_loss=tensor(0.1091, device='cuda:0') eval_ppl=tensor(1.1092, device='cuda:0') eval_epoch_loss=tensor(0.1036, device='cuda:0')\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/998 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f63ce679d4b8420fa206827a396e82c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5badc2c18c374fffb22b428c2ae737dd"}},"metadata":{}},{"name":"stdout","text":"epoch=3: train_ppl=tensor(1.1081, device='cuda:0') train_epoch_loss=tensor(0.1027, device='cuda:0') eval_ppl=tensor(1.1045, device='cuda:0') eval_epoch_loss=tensor(0.0994, device='cuda:0')\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/998 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64bad7b32c9042f79eb3a9608c87df50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0eea40919854951b4e656ee1201dc30"}},"metadata":{}},{"name":"stdout","text":"epoch=4: train_ppl=tensor(1.1024, device='cuda:0') train_epoch_loss=tensor(0.0974, device='cuda:0') eval_ppl=tensor(1.1023, device='cuda:0') eval_epoch_loss=tensor(0.0974, device='cuda:0')\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let’s see how well the model performs on the validation set:\r\n","metadata":{}},{"cell_type":"code","source":"correct = 0\ntotal = 0\nfor pred, true in zip(eval_preds, dataset[\"test\"][\"text_label\"]):\n    if pred.strip() == true.strip():\n        correct += 1\n    total += 1\naccuracy = correct / total * 100\nprint(f\"{accuracy=} % on the evaluation dataset\")\nprint(f\"{eval_preds[:10]=}\")\nprint(f\"{dataset['test']['text_label'][:10]=}\")","metadata":{"execution":{"iopub.status.busy":"2024-01-11T09:21:32.696096Z","iopub.execute_input":"2024-01-11T09:21:32.696382Z","iopub.status.idle":"2024-01-11T09:21:32.709927Z","shell.execute_reply.started":"2024-01-11T09:21:32.696357Z","shell.execute_reply":"2024-01-11T09:21:32.709173Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"accuracy=80.61122244488978 % on the evaluation dataset\neval_preds[:10]=['Hate Speech', 'Non-Hate', 'Non-Hate', 'Non-Hate', 'Non-Hate', 'Hate Speech', 'Hat-Hate', 'Non-Hate', 'Hate Speech', 'Hate Speech']\ndataset['test']['text_label'][:10]=['Hate Speech', 'Non-Hate', 'Non-Hate', 'Non-Hate', 'Non-Hate', 'Hate Speech', 'Non-Hate', 'Non-Hate', 'Hate Speech', 'Hate Speech']\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-01-11T09:21:32.710985Z","iopub.execute_input":"2024-01-11T09:21:32.711251Z","iopub.status.idle":"2024-01-11T09:21:32.746720Z","shell.execute_reply.started":"2024-01-11T09:21:32.711228Z","shell.execute_reply":"2024-01-11T09:21:32.745936Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"967139e5875644ae8550d2abdd03b7e3"}},"metadata":{}}]},{"cell_type":"code","source":"peft_model_id = \"omarxadel/flan-t5-xl-hs-prefix-tuning\"\nmodel.push_to_hub(peft_model_id, use_auth_token=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T09:23:28.311941Z","iopub.execute_input":"2024-01-11T09:23:28.312307Z","iopub.status.idle":"2024-01-11T09:23:30.225364Z","shell.execute_reply.started":"2024-01-11T09:23:28.312276Z","shell.execute_reply":"2024-01-11T09:23:30.224450Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:821: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/7.86M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe2e3f41e1924ad381690ebeb49d6943"}},"metadata":{}},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/omarxadel/flan-t5-xl-hs-prefix-tuning/commit/9b62e696fcbfd57c4a1c76ebc0b82daff4aa6e28', commit_message='Upload model', commit_description='', oid='9b62e696fcbfd57c4a1c76ebc0b82daff4aa6e28', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]}]}