{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7345136,"sourceType":"datasetVersion","datasetId":4265092}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# FLAN-T5 Prefix-Tuning\nThis notebook is made to prefix-tune FLAN-T5 on Hateeval.","metadata":{}},{"cell_type":"markdown","source":"### 1. Imports and Constants","metadata":{}},{"cell_type":"code","source":"import torch\ntorch.cuda.is_available()","metadata":{"execution":{"iopub.status.busy":"2024-01-09T22:14:56.657563Z","iopub.execute_input":"2024-01-09T22:14:56.657839Z","iopub.status.idle":"2024-01-09T22:15:00.143053Z","shell.execute_reply.started":"2024-01-09T22:14:56.657813Z","shell.execute_reply":"2024-01-09T22:15:00.142099Z"},"trusted":true},"execution_count":1,"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"!pip install peft","metadata":{"execution":{"iopub.status.busy":"2024-01-09T22:15:00.144995Z","iopub.execute_input":"2024-01-09T22:15:00.145407Z","iopub.status.idle":"2024-01-09T22:15:13.315431Z","shell.execute_reply.started":"2024-01-09T22:15:00.145380Z","shell.execute_reply":"2024-01-09T22:15:13.314443Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting peft\n  Obtaining dependency information for peft from https://files.pythonhosted.org/packages/8b/1b/aee2a330d050c493642d59ba6af51f3910cb138ea48ede228c84c204a5af/peft-0.7.1-py3-none-any.whl.metadata\n  Downloading peft-0.7.1-py3-none-any.whl.metadata (25 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.1)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.0.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.36.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.1)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.25.0)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.1)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.19.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.12.2)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2023.12.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.0.9)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.8.8)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.15.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2023.11.17)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.7.1-py3-none-any.whl (168 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.7.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, default_data_collator, get_linear_schedule_with_warmup\nfrom peft import get_peft_config, get_peft_model, get_peft_model_state_dict, PrefixTuningConfig, TaskType\nimport os\nfrom datasets import Dataset,DatasetDict, load_dataset\nfrom torch.utils.data import DataLoader\nimport torch\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom datasets import Features, ClassLabel\n\nfeatures = ['Non-Hate','Hate']\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n\ndevice = \"cuda\"\nmodel_name_or_path = \"google/flan-t5-xl\"\ntokenizer_name_or_path = \"google/flan-t5-xl\"\n\ntext_column = \"text\"\nlabel_column = \"text_label\"\nmax_length = 128\nlr = 1e-2\nnum_epochs = 5\nbatch_size = 8","metadata":{"execution":{"iopub.status.busy":"2024-01-09T22:15:13.316813Z","iopub.execute_input":"2024-01-09T22:15:13.317124Z","iopub.status.idle":"2024-01-09T22:15:26.536135Z","shell.execute_reply.started":"2024-01-09T22:15:13.317097Z","shell.execute_reply":"2024-01-09T22:15:26.535365Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 2. Loading Dataset","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/hateeval/hateeval_preprocessed_data.csv')\ndf = df.dropna()\ntrain, test = train_test_split(df, test_size=0.2)\ntrain = Dataset.from_pandas(train)\ntest = Dataset.from_pandas(test)\nds_dict = {'train' : train,\n           'test' : test}\ndataset = DatasetDict(ds_dict)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T22:15:26.538915Z","iopub.execute_input":"2024-01-09T22:15:26.540024Z","iopub.status.idle":"2024-01-09T22:15:26.638835Z","shell.execute_reply.started":"2024-01-09T22:15:26.539984Z","shell.execute_reply":"2024-01-09T22:15:26.637920Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### 3. Preprocessing Data\nInitialize a tokenizer, and create a function to pad and truncate the model_inputs and labels:","metadata":{}},{"cell_type":"code","source":"dataset = dataset.map(\n    lambda x: {\"text_label\": [features[label] for label in x[\"HS\"]]},\n    batched=True,\n    num_proc=1,\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T22:15:26.639838Z","iopub.execute_input":"2024-01-09T22:15:26.640115Z","iopub.status.idle":"2024-01-09T22:15:26.778828Z","shell.execute_reply.started":"2024-01-09T22:15:26.640092Z","shell.execute_reply":"2024-01-09T22:15:26.777989Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/8 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91148eeb9d2e416ea42b406850dba9ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6000916c04b4f10857da6de35f427ed"}},"metadata":{}}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n\n\ndef preprocess_function(examples):\n    inputs = examples[text_column]\n    targets = examples[label_column]\n    model_inputs = tokenizer(inputs, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n    model_inputs['input_ids'] = model_inputs['input_ids'].reshape(max_length)\n    labels = tokenizer(targets, max_length=5, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n    labels = labels[\"input_ids\"]\n    labels[labels == tokenizer.pad_token_id] = -100\n    model_inputs[\"labels\"] = labels.reshape(5)\n    return model_inputs","metadata":{"execution":{"iopub.status.busy":"2024-01-09T22:15:26.780129Z","iopub.execute_input":"2024-01-09T22:15:26.780477Z","iopub.status.idle":"2024-01-09T22:15:28.006490Z","shell.execute_reply.started":"2024-01-09T22:15:26.780445Z","shell.execute_reply":"2024-01-09T22:15:28.005548Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"faf2e2e5cde145fa8d9376731057ed26"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7cd06b7010e94e03895e1e0cdcfb3e8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd70d3bb7c1045eaaf0166ff6e49554a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7db6de2e07f9411db67918fd587dce96"}},"metadata":{}}]},{"cell_type":"markdown","source":"Use the map function to apply the preprocess_function to the dataset. You can remove the unprocessed columns since the model doesn’t need them anymore:","metadata":{}},{"cell_type":"code","source":"processed_datasets = dataset.map(\n    preprocess_function,\n    num_proc=1,\n    remove_columns=dataset[\"train\"].column_names,\n    load_from_cache_file=False,\n    desc=\"Running tokenizer on dataset\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T22:15:28.007765Z","iopub.execute_input":"2024-01-09T22:15:28.008392Z","iopub.status.idle":"2024-01-09T22:15:35.090932Z","shell.execute_reply.started":"2024-01-09T22:15:28.008349Z","shell.execute_reply":"2024-01-09T22:15:35.089943Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Running tokenizer on dataset:   0%|          | 0/7982 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef5742d2932a420585ce6dc18ebf811b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on dataset:   0%|          | 0/1996 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96594f46c8a14e1e8aa92833ccf8e64a"}},"metadata":{}}]},{"cell_type":"markdown","source":"Create a DataLoader from the train and eval datasets. Set pin_memory=True to speed up the data transfer to the GPU during training if the samples in your dataset are on a CPU.","metadata":{}},{"cell_type":"code","source":"train_dataset = processed_datasets[\"train\"]\neval_dataset = processed_datasets[\"test\"]\ntrain_dataloader = DataLoader(\n    train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True\n)\neval_dataloader = DataLoader(eval_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T22:15:35.092089Z","iopub.execute_input":"2024-01-09T22:15:35.092376Z","iopub.status.idle":"2024-01-09T22:15:35.097861Z","shell.execute_reply.started":"2024-01-09T22:15:35.092352Z","shell.execute_reply":"2024-01-09T22:15:35.096973Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### 4. Train model\nNow you can setup your model and make sure it is ready for training. Specify the task in PrefixTuningConfig, create the base t5-large model from AutoModelForSeq2SeqLM, and then wrap the model and configuration in a PeftModel. Feel free to print the PeftModel’s parameters and compare it to fully training all the model parameters to see how much more efficient it is!","metadata":{}},{"cell_type":"code","source":"peft_config = PrefixTuningConfig(task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, num_virtual_tokens=20)\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-01-09T22:15:35.098924Z","iopub.execute_input":"2024-01-09T22:15:35.099169Z","iopub.status.idle":"2024-01-09T22:20:31.508774Z","shell.execute_reply.started":"2024-01-09T22:15:35.099147Z","shell.execute_reply":"2024-01-09T22:20:31.507807Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.44k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ad25fd831334523a77e963d59dcfef0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/53.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec0f31648d6b4b92814b357ad9ac22b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59105058891a4c38b3c38f1bab8fff73"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.45G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"186c4228650d48f892c160d81320bf49"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01ed098d64554a90a18bf30aba53574b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e61baf0afe284de5926501b881d7eb08"}},"metadata":{}},{"name":"stdout","text":"trainable params: 1,966,080 || all params: 2,851,723,264 || trainable%: 0.06894357614638445\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Setup the optimizer and learning rate scheduler:\r\n","metadata":{}},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\nlr_scheduler = get_linear_schedule_with_warmup(\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=(len(train_dataloader) * num_epochs),\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T22:20:31.511450Z","iopub.execute_input":"2024-01-09T22:20:31.511734Z","iopub.status.idle":"2024-01-09T22:20:31.521353Z","shell.execute_reply.started":"2024-01-09T22:20:31.511703Z","shell.execute_reply":"2024-01-09T22:20:31.519958Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Move the model to the GPU, and then write a training loop to begin!\r\n","metadata":{}},{"cell_type":"code","source":"model = model.to(device)\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n    for step, batch in enumerate(tqdm(train_dataloader)):\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        total_loss += loss.detach().float()\n        loss.backward()\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n\n    model.eval()\n    eval_loss = 0\n    eval_preds = []\n    for step, batch in enumerate(tqdm(eval_dataloader)):\n        batch = {k: v.to(device) for k, v in batch.items()}\n        with torch.no_grad():\n            outputs = model(**batch)\n        loss = outputs.loss\n        eval_loss += loss.detach().float()\n        eval_preds.extend(\n            tokenizer.batch_decode(torch.argmax(outputs.logits, -1).detach().cpu().numpy(), skip_special_tokens=True)\n        )\n\n    eval_epoch_loss = eval_loss / len(eval_dataloader)\n    eval_ppl = torch.exp(eval_epoch_loss)\n    train_epoch_loss = total_loss / len(train_dataloader)\n    train_ppl = torch.exp(train_epoch_loss)\n    print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}\")","metadata":{"execution":{"iopub.status.busy":"2024-01-09T22:20:31.525363Z","iopub.execute_input":"2024-01-09T22:20:31.525592Z","iopub.status.idle":"2024-01-09T23:11:08.858361Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/998 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e722b0f578b47e9bb5db54c863d0746"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc97cbf454d84f3983de826031863c66"}},"metadata":{}},{"name":"stdout","text":"epoch=0: train_ppl=tensor(1.2304, device='cuda:0') train_epoch_loss=tensor(0.2073, device='cuda:0') eval_ppl=tensor(1.1338, device='cuda:0') eval_epoch_loss=tensor(0.1256, device='cuda:0')\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/998 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64ba2472510b42d1be681a0147912f6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d45c38688b294a2cbff240acb93e8c68"}},"metadata":{}},{"name":"stdout","text":"epoch=1: train_ppl=tensor(1.1428, device='cuda:0') train_epoch_loss=tensor(0.1335, device='cuda:0') eval_ppl=tensor(1.1177, device='cuda:0') eval_epoch_loss=tensor(0.1112, device='cuda:0')\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/998 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c139e66efa140e4ae5a83b22516f23f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"541e3b1855234defaee6bdd65aa7754b"}},"metadata":{}},{"name":"stdout","text":"epoch=2: train_ppl=tensor(1.1257, device='cuda:0') train_epoch_loss=tensor(0.1184, device='cuda:0') eval_ppl=tensor(1.1100, device='cuda:0') eval_epoch_loss=tensor(0.1043, device='cuda:0')\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/998 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0aef22fc9fdd4558a09bb55615efc9f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b35cdf4fb5948b099113fe8a092b23e"}},"metadata":{}},{"name":"stdout","text":"epoch=3: train_ppl=tensor(1.1189, device='cuda:0') train_epoch_loss=tensor(0.1124, device='cuda:0') eval_ppl=tensor(1.1148, device='cuda:0') eval_epoch_loss=tensor(0.1087, device='cuda:0')\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/998 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e2cf6b751f8406dbb5b80ee891d88de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9675d3f182c74c92bfc43d37f90b87d8"}},"metadata":{}},{"name":"stdout","text":"epoch=4: train_ppl=tensor(1.1147, device='cuda:0') train_epoch_loss=tensor(0.1086, device='cuda:0') eval_ppl=tensor(1.1092, device='cuda:0') eval_epoch_loss=tensor(0.1036, device='cuda:0')\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let’s see how well the model performs on the validation set:\r\n","metadata":{}},{"cell_type":"code","source":"correct = 0\ntotal = 0\nfor pred, true in zip(eval_preds, dataset[\"test\"][\"text_label\"]):\n    if pred.strip() == true.strip():\n        correct += 1\n    total += 1\naccuracy = correct / total * 100\nprint(f\"{accuracy=} % on the evaluation dataset\")\nprint(f\"{eval_preds[:10]=}\")\nprint(f\"{dataset['test']['text_label'][:10]=}\")","metadata":{"execution":{"iopub.status.busy":"2024-01-09T23:11:08.859582Z","iopub.execute_input":"2024-01-09T23:11:08.859872Z","iopub.status.idle":"2024-01-09T23:11:08.874696Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"accuracy=44.83967935871743 % on the evaluation dataset\neval_preds[:10]=['Non-Hate', 'Hat-Hate', 'Hate Hat', 'Non-Hate', 'Hat-Hate', 'Non-Hate', 'Hate Non Hat', 'Hate Non Hat', 'Non-Hate', 'Non-Hate']\ndataset['test']['text_label'][:10]=['Non-Hate', 'Non-Hate', 'Hate', 'Non-Hate', 'Non-Hate', 'Non-Hate', 'Hate', 'Hate', 'Non-Hate', 'Non-Hate']\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-01-09T23:11:08.875668Z","iopub.execute_input":"2024-01-09T23:11:08.875906Z","iopub.status.idle":"2024-01-09T23:11:08.920203Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"303810f5b3c942ff94ca7b51eb4c32eb"}},"metadata":{}}]},{"cell_type":"code","source":"peft_model_id = \"omarxadel/flan-t5-xl-hs-prefix-tuning\"\nmodel.push_to_hub(peft_model_id, use_auth_token=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T23:17:41.101811Z","iopub.execute_input":"2024-01-09T23:17:41.102566Z","iopub.status.idle":"2024-01-09T23:17:42.942812Z","shell.execute_reply.started":"2024-01-09T23:17:41.102530Z","shell.execute_reply":"2024-01-09T23:17:42.941767Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:821: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/7.86M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"156ad3ef5a55489085d02d0b1915319c"}},"metadata":{}},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/omarxadel/flan-t5-xl-hs-prefix-tuning/commit/f8ce817c27c5cbfd1d9fcf69e4e362fb70bb8d11', commit_message='Upload model', commit_description='', oid='f8ce817c27c5cbfd1d9fcf69e4e362fb70bb8d11', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]}]}